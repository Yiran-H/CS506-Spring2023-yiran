{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 12\n",
    "\n",
    "Name: Yiran Huang\n",
    "UID: U71885452\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Naive Bayes\n",
    "- Model Evaluation\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "| Attribute A | Attribute B | Attribute C | Class |\n",
    "|-------------|-------------|-------------|-------|\n",
    "| Yes         | Single      | High        | No    |\n",
    "| No          | Married     | Mid         | No    |\n",
    "| No          | Single      | Low         | No    |\n",
    "| Yes         | Married     | High        | No    |\n",
    "| No          | Divorced    | Mid         | Yes   |\n",
    "| No          | Married     | Low         | No    |\n",
    "| Yes         | Divorced    | High        | No    |\n",
    "| No          | Single      | Mid         | Yes   |\n",
    "| No          | Married     | Low         | No    |\n",
    "| No          | Single      | Mid         | Yes   |\n",
    "\n",
    "a) Compute the following probabilities:\n",
    "\n",
    "- P(Attribute A = Yes | Class = No)\n",
    "- P(Attribute B = Divorced | Class = Yes)\n",
    "- P(Attribute C = High | Class = No)\n",
    "- P(Attribute C = Mid | Class = Yes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P(Attribute A = Yes | Class = No) = 3/7\n",
    "- P(Attribute B = Divorced | Class = Yes) = 1/3\n",
    "- P(Attribute C = High | Class = No) = 3/7\n",
    "- P(Attribute C = Mid | Class = Yes) = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Classify the following unseen records:\n",
    "\n",
    "- (Yes, Married, Mid)\n",
    "- (No, Divorced, High)\n",
    "- (No, Single, High)\n",
    "- (No, Divorced, Low)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No NO NO NO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "a) Write a function to generate the confusion matrix for a list of actual classes and a list of predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 3.]\n",
      " [1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "actual_class = [\"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\"]\n",
    "predicted_class = [\"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\"]\n",
    "\n",
    "def confusion_matrix(actual, predicted):\n",
    "    classes = np.unique(actual)\n",
    "    # Initialize the confusion matrix as a 2D array of zeros\n",
    "    matrix = np.zeros((len(classes), len(classes)))\n",
    "    # For each pair of actual and predicted classes, increment the corresponding cell in the matrix\n",
    "    for i in range(len(actual)):\n",
    "        true_idx = np.where(classes == actual[i])[0][0]\n",
    "        pred_idx = np.where(classes == predicted[i])[0][0]\n",
    "        matrix[true_idx, pred_idx] += 1\n",
    "    return matrix\n",
    "\n",
    "print(confusion_matrix(actual_class, predicted_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Assume you have the following Cost Matrix:\n",
    "\n",
    "|            | predicted = Y | predicted = N |\n",
    "|------------|---------------|---------------|\n",
    "| actual = Y |       -1      |       5       |\n",
    "| actual = N |        10     |       0       |\n",
    "\n",
    "What is the cost of the above classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_matrix = np.array([[-1, 5], [10, 0]])\n",
    "confusion_matrix_m = np.array([[4, 3], [1, 2]])\n",
    "cost = np.sum(confusion_matrix_m * cost_matrix)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Write a function that takes in the actual values, the predictions, and a cost matrix and outputs a cost. Test it on the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_score(act,pred,ct_matrix):\n",
    "    conf_matrix=confusion_matrix(act,pred)\n",
    "    cost = np.sum(conf_matrix * ct_matrix)\n",
    "    return cost\n",
    "\n",
    "get_score(actual_class,predicted_class,cost_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) What is the difference between a testing set and a validation set?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the validation set is used to tune the model, while the testing set is used to evaluate the final performance of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) What are some things you can do to handle the case where you have a highly imbalanced dataset (i.e. there are way more of one class than another). Describe both how you can provide better visibility into the failures of the model and how you can set your model training up for success."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Resampling: One way to handle imbalanced datasets is to perform data resampling. This can be done either by oversampling the minority class or undersampling the majority class. Oversampling can be done by replicating the samples of the minority class, while undersampling can be done by removing samples from the majority class. However, it is important to ensure that the resampling is done in a way that does not introduce bias into the dataset.\n",
    "\n",
    "2. Data Augmentation: Another way to handle imbalanced datasets is to use data augmentation techniques. This involves creating new samples by applying transformations to the existing data. For example, in image classification tasks, you can use techniques like random cropping, flipping, and rotation to create new images from the existing ones.\n",
    "\n",
    "It is also important to use appropriate evaluation metrics like precision, recall, F1-score, and AUC-ROC, which are more suitable for imbalanced datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
